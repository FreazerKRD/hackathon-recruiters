{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим нужные библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ru-core-news-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.5.0/ru_core_news_sm-3.5.0-py3-none-any.whl (15.3 MB)\n",
      "                                              0.0/15.3 MB ? eta -:--:--\n",
      "                                              0.0/15.3 MB ? eta -:--:--\n",
      "                                              0.1/15.3 MB 1.1 MB/s eta 0:00:14\n",
      "                                              0.1/15.3 MB 1.2 MB/s eta 0:00:13\n",
      "                                              0.2/15.3 MB 1.1 MB/s eta 0:00:14\n",
      "                                              0.2/15.3 MB 1.4 MB/s eta 0:00:11\n",
      "                                              0.3/15.3 MB 1.5 MB/s eta 0:00:10\n",
      "     -                                        0.5/15.3 MB 1.7 MB/s eta 0:00:09\n",
      "     -                                        0.5/15.3 MB 1.8 MB/s eta 0:00:09\n",
      "     -                                        0.6/15.3 MB 1.9 MB/s eta 0:00:08\n",
      "     -                                        0.7/15.3 MB 1.9 MB/s eta 0:00:08\n",
      "     --                                       0.8/15.3 MB 2.0 MB/s eta 0:00:08\n",
      "     --                                       1.0/15.3 MB 2.1 MB/s eta 0:00:07\n",
      "     --                                       1.0/15.3 MB 2.1 MB/s eta 0:00:07\n",
      "     --                                       1.1/15.3 MB 2.1 MB/s eta 0:00:07\n",
      "     ---                                      1.2/15.3 MB 2.1 MB/s eta 0:00:07\n",
      "     ---                                      1.3/15.3 MB 2.2 MB/s eta 0:00:07\n",
      "     ---                                      1.4/15.3 MB 2.2 MB/s eta 0:00:07\n",
      "     ---                                      1.5/15.3 MB 2.2 MB/s eta 0:00:07\n",
      "     ----                                     1.6/15.3 MB 2.2 MB/s eta 0:00:07\n",
      "     ----                                     1.7/15.3 MB 2.2 MB/s eta 0:00:07\n",
      "     ----                                     1.8/15.3 MB 2.2 MB/s eta 0:00:07\n",
      "     ----                                     1.9/15.3 MB 2.2 MB/s eta 0:00:07\n",
      "     -----                                    2.0/15.3 MB 2.2 MB/s eta 0:00:06\n",
      "     -----                                    2.1/15.3 MB 2.2 MB/s eta 0:00:06\n",
      "     -----                                    2.2/15.3 MB 2.3 MB/s eta 0:00:06\n",
      "     -----                                    2.3/15.3 MB 2.3 MB/s eta 0:00:06\n",
      "     ------                                   2.4/15.3 MB 2.3 MB/s eta 0:00:06\n",
      "     ------                                   2.5/15.3 MB 2.3 MB/s eta 0:00:06\n",
      "     ------                                   2.6/15.3 MB 2.3 MB/s eta 0:00:06\n",
      "     -------                                  2.7/15.3 MB 2.3 MB/s eta 0:00:06\n",
      "     -------                                  2.8/15.3 MB 2.4 MB/s eta 0:00:06\n",
      "     -------                                  2.9/15.3 MB 2.4 MB/s eta 0:00:06\n",
      "     -------                                  3.0/15.3 MB 2.4 MB/s eta 0:00:06\n",
      "     --------                                 3.2/15.3 MB 2.4 MB/s eta 0:00:06\n",
      "     --------                                 3.3/15.3 MB 2.4 MB/s eta 0:00:06\n",
      "     --------                                 3.4/15.3 MB 2.4 MB/s eta 0:00:05\n",
      "     ---------                                3.5/15.3 MB 2.4 MB/s eta 0:00:05\n",
      "     ---------                                3.6/15.3 MB 2.4 MB/s eta 0:00:05\n",
      "     ---------                                3.6/15.3 MB 2.4 MB/s eta 0:00:05\n",
      "     ---------                                3.7/15.3 MB 2.4 MB/s eta 0:00:05\n",
      "     ----------                               3.8/15.3 MB 2.4 MB/s eta 0:00:05\n",
      "     ----------                               4.0/15.3 MB 2.4 MB/s eta 0:00:05\n",
      "     ----------                               4.1/15.3 MB 2.4 MB/s eta 0:00:05\n",
      "     ----------                               4.1/15.3 MB 2.4 MB/s eta 0:00:05\n",
      "     -----------                              4.3/15.3 MB 2.4 MB/s eta 0:00:05\n",
      "     -----------                              4.4/15.3 MB 2.4 MB/s eta 0:00:05\n",
      "     -----------                              4.5/15.3 MB 2.4 MB/s eta 0:00:05\n",
      "     ------------                             4.6/15.3 MB 2.5 MB/s eta 0:00:05\n",
      "     ------------                             4.7/15.3 MB 2.4 MB/s eta 0:00:05\n",
      "     ------------                             4.8/15.3 MB 2.5 MB/s eta 0:00:05\n",
      "     ------------                             4.9/15.3 MB 2.5 MB/s eta 0:00:05\n",
      "     -------------                            5.0/15.3 MB 2.5 MB/s eta 0:00:05\n",
      "     -------------                            5.1/15.3 MB 2.5 MB/s eta 0:00:05\n",
      "     -------------                            5.2/15.3 MB 2.5 MB/s eta 0:00:05\n",
      "     --------------                           5.4/15.3 MB 2.5 MB/s eta 0:00:04\n",
      "     --------------                           5.5/15.3 MB 2.5 MB/s eta 0:00:04\n",
      "     --------------                           5.6/15.3 MB 2.5 MB/s eta 0:00:04\n",
      "     --------------                           5.7/15.3 MB 2.5 MB/s eta 0:00:04\n",
      "     ---------------                          5.8/15.3 MB 2.5 MB/s eta 0:00:04\n",
      "     ---------------                          5.9/15.3 MB 2.5 MB/s eta 0:00:04\n",
      "     ---------------                          6.0/15.3 MB 2.5 MB/s eta 0:00:04\n",
      "     ----------------                         6.1/15.3 MB 2.5 MB/s eta 0:00:04\n",
      "     ----------------                         6.2/15.3 MB 2.5 MB/s eta 0:00:04\n",
      "     ----------------                         6.3/15.3 MB 2.5 MB/s eta 0:00:04\n",
      "     ----------------                         6.4/15.3 MB 2.5 MB/s eta 0:00:04\n",
      "     -----------------                        6.5/15.3 MB 2.5 MB/s eta 0:00:04\n",
      "     -----------------                        6.6/15.3 MB 2.5 MB/s eta 0:00:04\n",
      "     -----------------                        6.7/15.3 MB 2.5 MB/s eta 0:00:04\n",
      "     -----------------                        6.9/15.3 MB 2.5 MB/s eta 0:00:04\n",
      "     ------------------                       6.9/15.3 MB 2.5 MB/s eta 0:00:04\n",
      "     ------------------                       7.0/15.3 MB 2.5 MB/s eta 0:00:04\n",
      "     ------------------                       7.1/15.3 MB 2.5 MB/s eta 0:00:04\n",
      "     ------------------                       7.2/15.3 MB 2.5 MB/s eta 0:00:04\n",
      "     -------------------                      7.3/15.3 MB 2.5 MB/s eta 0:00:04\n",
      "     -------------------                      7.4/15.3 MB 2.5 MB/s eta 0:00:04\n",
      "     -------------------                      7.4/15.3 MB 2.5 MB/s eta 0:00:04\n",
      "     -------------------                      7.6/15.3 MB 2.5 MB/s eta 0:00:04\n",
      "     --------------------                     7.7/15.3 MB 2.5 MB/s eta 0:00:04\n",
      "     --------------------                     7.8/15.3 MB 2.5 MB/s eta 0:00:03\n",
      "     --------------------                     7.8/15.3 MB 2.5 MB/s eta 0:00:03\n",
      "     --------------------                     7.9/15.3 MB 2.5 MB/s eta 0:00:03\n",
      "     ---------------------                    8.0/15.3 MB 2.5 MB/s eta 0:00:03\n",
      "     ---------------------                    8.1/15.3 MB 2.5 MB/s eta 0:00:03\n",
      "     ---------------------                    8.2/15.3 MB 2.5 MB/s eta 0:00:03\n",
      "     ---------------------                    8.3/15.3 MB 2.5 MB/s eta 0:00:03\n",
      "     ----------------------                   8.4/15.3 MB 2.5 MB/s eta 0:00:03\n",
      "     ----------------------                   8.5/15.3 MB 2.5 MB/s eta 0:00:03\n",
      "     ----------------------                   8.6/15.3 MB 2.5 MB/s eta 0:00:03\n",
      "     ----------------------                   8.7/15.3 MB 2.5 MB/s eta 0:00:03\n",
      "     -----------------------                  8.8/15.3 MB 2.5 MB/s eta 0:00:03\n",
      "     -----------------------                  8.8/15.3 MB 2.5 MB/s eta 0:00:03\n",
      "     -----------------------                  9.0/15.3 MB 2.5 MB/s eta 0:00:03\n",
      "     -----------------------                  9.1/15.3 MB 2.5 MB/s eta 0:00:03\n",
      "     ------------------------                 9.2/15.3 MB 2.5 MB/s eta 0:00:03\n",
      "     ------------------------                 9.3/15.3 MB 2.5 MB/s eta 0:00:03\n",
      "     ------------------------                 9.4/15.3 MB 2.5 MB/s eta 0:00:03\n",
      "     ------------------------                 9.5/15.3 MB 2.5 MB/s eta 0:00:03\n",
      "     -------------------------                9.6/15.3 MB 2.5 MB/s eta 0:00:03\n",
      "     -------------------------                9.7/15.3 MB 2.5 MB/s eta 0:00:03\n",
      "     -------------------------                9.8/15.3 MB 2.5 MB/s eta 0:00:03\n",
      "     -------------------------                9.9/15.3 MB 2.5 MB/s eta 0:00:03\n",
      "     --------------------------               10.0/15.3 MB 2.5 MB/s eta 0:00:03\n",
      "     --------------------------               10.1/15.3 MB 2.5 MB/s eta 0:00:03\n",
      "     --------------------------               10.2/15.3 MB 2.5 MB/s eta 0:00:03\n",
      "     --------------------------               10.3/15.3 MB 2.5 MB/s eta 0:00:02\n",
      "     ---------------------------              10.3/15.3 MB 2.5 MB/s eta 0:00:02\n",
      "     ---------------------------              10.4/15.3 MB 2.5 MB/s eta 0:00:02\n",
      "     ---------------------------              10.5/15.3 MB 2.5 MB/s eta 0:00:02\n",
      "     ---------------------------              10.6/15.3 MB 2.5 MB/s eta 0:00:02\n",
      "     ----------------------------             10.7/15.3 MB 2.5 MB/s eta 0:00:02\n",
      "     ----------------------------             10.8/15.3 MB 2.5 MB/s eta 0:00:02\n",
      "     ----------------------------             10.9/15.3 MB 2.5 MB/s eta 0:00:02\n",
      "     ----------------------------             11.0/15.3 MB 2.5 MB/s eta 0:00:02\n",
      "     -----------------------------            11.1/15.3 MB 2.5 MB/s eta 0:00:02\n",
      "     -----------------------------            11.2/15.3 MB 2.5 MB/s eta 0:00:02\n",
      "     -----------------------------            11.3/15.3 MB 2.5 MB/s eta 0:00:02\n",
      "     -----------------------------            11.4/15.3 MB 2.5 MB/s eta 0:00:02\n",
      "     ------------------------------           11.5/15.3 MB 2.5 MB/s eta 0:00:02\n",
      "     ------------------------------           11.6/15.3 MB 2.5 MB/s eta 0:00:02\n",
      "     ------------------------------           11.7/15.3 MB 2.5 MB/s eta 0:00:02\n",
      "     ------------------------------           11.8/15.3 MB 2.5 MB/s eta 0:00:02\n",
      "     -------------------------------          11.9/15.3 MB 2.5 MB/s eta 0:00:02\n",
      "     -------------------------------          12.0/15.3 MB 2.5 MB/s eta 0:00:02\n",
      "     -------------------------------          12.1/15.3 MB 2.6 MB/s eta 0:00:02\n",
      "     -------------------------------          12.1/15.3 MB 2.5 MB/s eta 0:00:02\n",
      "     -------------------------------          12.1/15.3 MB 2.5 MB/s eta 0:00:02\n",
      "     -------------------------------          12.1/15.3 MB 2.5 MB/s eta 0:00:02\n",
      "     --------------------------------         12.3/15.3 MB 2.5 MB/s eta 0:00:02\n",
      "     --------------------------------         12.6/15.3 MB 2.5 MB/s eta 0:00:02\n",
      "     ---------------------------------        12.7/15.3 MB 2.5 MB/s eta 0:00:02\n",
      "     ---------------------------------        12.8/15.3 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------        12.9/15.3 MB 2.5 MB/s eta 0:00:01\n",
      "     ----------------------------------       13.0/15.3 MB 2.5 MB/s eta 0:00:01\n",
      "     ----------------------------------       13.1/15.3 MB 2.5 MB/s eta 0:00:01\n",
      "     ----------------------------------       13.2/15.3 MB 2.5 MB/s eta 0:00:01\n",
      "     ----------------------------------       13.3/15.3 MB 2.5 MB/s eta 0:00:01\n",
      "     -----------------------------------      13.4/15.3 MB 2.5 MB/s eta 0:00:01\n",
      "     -----------------------------------      13.4/15.3 MB 2.5 MB/s eta 0:00:01\n",
      "     -----------------------------------      13.5/15.3 MB 2.5 MB/s eta 0:00:01\n",
      "     -----------------------------------      13.6/15.3 MB 2.5 MB/s eta 0:00:01\n",
      "     -----------------------------------      13.7/15.3 MB 2.5 MB/s eta 0:00:01\n",
      "     ------------------------------------     13.8/15.3 MB 2.5 MB/s eta 0:00:01\n",
      "     ------------------------------------     13.9/15.3 MB 2.5 MB/s eta 0:00:01\n",
      "     ------------------------------------     14.0/15.3 MB 2.5 MB/s eta 0:00:01\n",
      "     ------------------------------------     14.1/15.3 MB 2.5 MB/s eta 0:00:01\n",
      "     -------------------------------------    14.2/15.3 MB 2.5 MB/s eta 0:00:01\n",
      "     -------------------------------------    14.3/15.3 MB 2.5 MB/s eta 0:00:01\n",
      "     -------------------------------------    14.4/15.3 MB 2.5 MB/s eta 0:00:01\n",
      "     -------------------------------------    14.5/15.3 MB 2.5 MB/s eta 0:00:01\n",
      "     --------------------------------------   14.6/15.3 MB 2.5 MB/s eta 0:00:01\n",
      "     --------------------------------------   14.6/15.3 MB 2.5 MB/s eta 0:00:01\n",
      "     --------------------------------------   14.8/15.3 MB 2.5 MB/s eta 0:00:01\n",
      "     --------------------------------------   14.8/15.3 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  14.9/15.3 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  15.0/15.3 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  15.1/15.3 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  15.2/15.3 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  15.2/15.3 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  15.2/15.3 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  15.2/15.3 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  15.2/15.3 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  15.2/15.3 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  15.2/15.3 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  15.2/15.3 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  15.2/15.3 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  15.2/15.3 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 15.3/15.3 MB 2.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\андрей\\desktop\\github\\recruiters\\venv\\lib\\site-packages (from ru-core-news-sm==3.5.0) (3.5.4)\n",
      "Requirement already satisfied: pymorphy3>=1.0.0 in c:\\users\\андрей\\desktop\\github\\recruiters\\venv\\lib\\site-packages (from ru-core-news-sm==3.5.0) (1.2.0)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in c:\\users\\андрей\\desktop\\github\\recruiters\\venv\\lib\\site-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.5.0) (0.7.2)\n",
      "Requirement already satisfied: docopt>=0.6 in c:\\users\\андрей\\desktop\\github\\recruiters\\venv\\lib\\site-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.5.0) (0.6.2)\n",
      "Requirement already satisfied: pymorphy3-dicts-ru in c:\\users\\андрей\\desktop\\github\\recruiters\\venv\\lib\\site-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.5.0) (2.4.417150.4580142)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\андрей\\desktop\\github\\recruiters\\venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\андрей\\desktop\\github\\recruiters\\venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\андрей\\desktop\\github\\recruiters\\venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\андрей\\desktop\\github\\recruiters\\venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\андрей\\desktop\\github\\recruiters\\venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\андрей\\desktop\\github\\recruiters\\venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\андрей\\desktop\\github\\recruiters\\venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\андрей\\desktop\\github\\recruiters\\venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\андрей\\desktop\\github\\recruiters\\venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\андрей\\desktop\\github\\recruiters\\venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\андрей\\desktop\\github\\recruiters\\venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\андрей\\desktop\\github\\recruiters\\venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\андрей\\desktop\\github\\recruiters\\venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\андрей\\desktop\\github\\recruiters\\venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (1.25.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\андрей\\desktop\\github\\recruiters\\venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\андрей\\desktop\\github\\recruiters\\venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (1.10.10)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\андрей\\desktop\\github\\recruiters\\venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\андрей\\desktop\\github\\recruiters\\venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (63.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\андрей\\desktop\\github\\recruiters\\venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\андрей\\desktop\\github\\recruiters\\venv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\андрей\\desktop\\github\\recruiters\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\андрей\\desktop\\github\\recruiters\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\андрей\\desktop\\github\\recruiters\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\андрей\\desktop\\github\\recruiters\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\андрей\\desktop\\github\\recruiters\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (2023.5.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\андрей\\desktop\\github\\recruiters\\venv\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\андрей\\desktop\\github\\recruiters\\venv\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (0.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\андрей\\desktop\\github\\recruiters\\venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\андрей\\desktop\\github\\recruiters\\venv\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\андрей\\desktop\\github\\recruiters\\venv\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (2.1.3)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('ru_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy download ru_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "import spacy\n",
    "import json\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "import json\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('csv\\\\full_df.csv', sep=';')\n",
    "data.head(3)\n",
    "data = data.drop(columns='Unnamed: 0', axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отфильтруем тексты по количеству символов. Оставим только 100+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_len'] = data.loc[:, 'text'].apply(lambda x: len(x))\n",
    "data_filtered_by_text_len = data.query('text_len > 100')\n",
    "print(f'Количество постов с 100+ символами: {data_filtered_by_text_len.shape[0]}')\n",
    "data_filtered_by_text_len.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оставим только тексты содержащие кириллицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cyrillic = data_filtered_by_text_len[data_filtered_by_text_len['text'].apply(lambda x: re.match(r'[А-Яа-я]+', x) is not None)]\n",
    "\n",
    "print(f'Текстов на кириллице: {data_cyrillic.shape[0]}')\n",
    "data_cyrillic.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалим дубликаты текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dd = data_cyrillic.drop_duplicates('text')\n",
    "print(f'Осталось {data_dd.shape[0]} строк')\n",
    "data_dd.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка к моделированию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data_dd[\"text\"].tolist()\n",
    "texts[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведем лемматизацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('lemmas.json'):\n",
    "    with open(\"lemmas.json\") as f:\n",
    "        data_lemmatized = json.load(f)['lemmas']\n",
    "else:\n",
    "    model = spacy.load('ru_core_news_sm', disable=['ner', 'parser'])\n",
    "    data_lemmatized = []\n",
    "    for doc in model.pipe(texts, disable=[\"tagger\", \"parser\"]):\n",
    "        data_lemmatized.append([token.lemma_ for token in doc])\n",
    "\n",
    "    with open(\"lemmas.json\", \"w\") as fid:\n",
    "        json.dump({\"lemmas\": data_lemmatized}, fid)\n",
    "\n",
    "data_lemmatized[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очистим тексты от ненужных символов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pattern = re.compile(\"^[а-я]*$\")\n",
    "\n",
    "def remove_symbols(doc):\n",
    "    return [token for token in doc if word_pattern.match(token)]\n",
    "\n",
    "data_words = list(map(remove_symbols, data_lemmatized))\n",
    "data_words[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим русские стоп-слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('russian')\n",
    "stop_words += ['это', 'свой', 'очень', 'мочь', 'ваш', 'наш']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим функцию для удаления стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in doc if word not in stop_words] for doc in texts]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим функцию для удаления стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words_nostops = remove_stopwords(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(data_words_nostops)\n",
    "\n",
    "texts = data_words_nostops\n",
    "\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Протестируем моделирование с 20 темами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    num_topics=20, \n",
    "    random_state=100,\n",
    "    update_every=1,\n",
    "    chunksize=100,\n",
    "    passes=10,\n",
    "    alpha='auto',\n",
    "    per_word_topics=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.print_topics()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислим сложность и согласованность модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Perplexity: ', lda_model.log_perplexity(corpus))\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score: ', coherence_lda)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отобразим результаты моделирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подбор количества тем"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подберем количество тем основываясь на согласованности модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start, step):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.ldamodel.LdaModel(\n",
    "            corpus=corpus,\n",
    "            id2word=id2word,\n",
    "            num_topics=num_topics, \n",
    "            random_state=765,\n",
    "            update_every=1,\n",
    "            chunksize=100,\n",
    "            passes=10,\n",
    "            alpha='auto',\n",
    "            per_word_topics=True\n",
    "            )\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_words_nostops, start=1, limit=10, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(1,10,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit=10\n",
    "start=1\n",
    "step=1\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наилучший результат показала модель с 3 темами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_scorer(data):\n",
    "    id2word = corpora.Dictionary(data)\n",
    "\n",
    "    texts = data\n",
    "\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=id2word,\n",
    "        num_topics=3, \n",
    "        random_state=765,\n",
    "        update_every=1,\n",
    "        chunksize=100,\n",
    "        passes=10,\n",
    "        alpha='auto',\n",
    "        per_word_topics=True\n",
    "        )\n",
    "\n",
    "    print('Perplexity: ', lda_model.log_perplexity(corpus))\n",
    "\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=id2word, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print('Coherence Score: ', coherence_lda)\n",
    "\n",
    "    return lda_model, corpus, id2word\n",
    "\n",
    "best_model, best_corpus, best_id2word = model_scorer(data_words_nostops)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Использование биграмм и триграмм"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, повлияет ли использование биграмм и триграмм на согласованность модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(data_words_nostops, min_count=5, threshold=100)\n",
    "trigram = gensim.models.Phrases(bigram[data_words_nostops], threshold=100)  \n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "data_words_trigrams = make_trigrams(data_words_nostops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scorer(data_words_bigrams)\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scorer(data_words_trigrams)\n",
    "print('Complete')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использование биграмм и триграмм только ухудшили согласованнось"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ наилучшей модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(best_model, best_corpus, best_id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_probs = best_model.get_document_topics(best_corpus)\n",
    "max_prob_topics = list(map(lambda doc_probs: max(doc_probs, key=lambda x: x[1])[0], topic_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([list(data_dd[\"text\"]), max_prob_topics]).transpose()\n",
    "df.columns = ['text', 'topic_label']\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['topic_label'] == 1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.show_topic(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вывод"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С имеющимися данными наилучшую согласованность имеет модель с разделением на 3 темы. После разделения на 3 темы было выявлено, что как отдельная тема выделены тексты на украинском языке. Для получения нормальных результатов нужно провести повторное моделирование исключив из данных украинские тексты."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
