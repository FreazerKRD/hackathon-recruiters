{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811098ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "328f7ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b996d87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция авторизации\n",
    "def auth(driver):\n",
    "    # Авторизация по cookies\n",
    "    if os.path.exists(FILES_PATH + COOKIES_PATH):\n",
    "        driver.get('https://linkedin.com')\n",
    "        #Загрузка куки\n",
    "        for cookie in pickle.load(open(FILES_PATH + COOKIES_PATH, 'rb')):\n",
    "            driver.add_cookie(cookie)\n",
    "        time.sleep(random.uniform(.5, 1))\n",
    "        driver.refresh()\n",
    "        time.sleep(random.uniform(.5, 2))\n",
    "\n",
    "    # Вход по паролю и сохранение cookies\n",
    "    else:\n",
    "        driver.get('https://linkedin.com/uas/login')\n",
    "        #Авторизация\n",
    "        time.sleep(random.uniform(2, 4))\n",
    "        username = driver.find_element(By.ID, \"username\")\n",
    "        username.send_keys(str(input('Введите логин: ')))\n",
    "        pword = driver.find_element(By.ID, \"password\")\n",
    "        pword.send_keys(str(input('Введите пароль: ')))\n",
    "        driver.find_element(By.XPATH, \"//button[@type='submit']\").click()\n",
    "        time.sleep(30) #время на ввод кода подтверждения если понадобится\n",
    "        pickle.dump(driver.get_cookies(), open(FILES_PATH + COOKIES_PATH, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cded2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для получения URL страниц пользователей\n",
    "def profile_urls(driver, search_url):\n",
    "    # Загрузка страницы с результатами поиска\n",
    "    driver.get(search_url)\n",
    "    time.sleep(random.uniform(.5, 3))\n",
    "\n",
    "    # Необходимые переменные\n",
    "    profile_urls = []\n",
    "\n",
    "    for i in range(NUM_PAGES_TO_PARSE):\n",
    "        # Текущая страница\n",
    "        print('Page', i+1)\n",
    "        # Задержка для полной загрузки страницы\n",
    "        time.sleep(random.uniform(4, 7))\n",
    "\n",
    "        search_result_links = driver.find_elements(By.CSS_SELECTOR, \"span.entity-result__title-text a.app-aware-link\")\n",
    "\n",
    "        for link in search_result_links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if 'linkedin.com/in' in href:\n",
    "                profile_urls.append(href)\n",
    "\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        NUM_SCROLLS = 20\n",
    "\n",
    "        for _ in range(NUM_SCROLLS):\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(random.uniform(1.5, 3))\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "\n",
    "        time.sleep(random.uniform(4, 7))\n",
    "\n",
    "        print('Sucсess!')\n",
    "        print('-' * 50)\n",
    "\n",
    "        next_button = driver.find_element(By.CSS_SELECTOR, 'button.artdeco-pagination__button--next')\n",
    "        next_button.click()\n",
    "\n",
    "\n",
    "    profile_urls = pd.Series(profile_urls)\n",
    "    profile_urls_cut = profile_urls.str.split('?', n=1).str[0]\n",
    "    print('Done!')\n",
    "\n",
    "    # Сохранение файла с URL\n",
    "    profile_urls_cut.to_csv(URL_FILE_NAME)\n",
    "    print(f'URLs saved to file: {URL_FILE_NAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4c1be79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_save_profile_info(driver, profile_url):\n",
    "    # This will open the link\n",
    "    driver.get(profile_url)\n",
    "    \n",
    "    # Random sleeping time to load all data\n",
    "    time.sleep(random.uniform(4, 7))\n",
    "\n",
    "    # Extracting data from page with BeautifulSoup\n",
    "    src = driver.page_source\n",
    "\n",
    "    # Now using beautiful soup    \n",
    "    soup = BeautifulSoup(src, 'lxml')\n",
    "\n",
    "    # Extracting the HTML of the complete introduction box\n",
    "    # that contains the name, status, and the location\n",
    "    intro = soup.find('div', {'class': 'pv-text-details__left-panel'})\n",
    "    \n",
    "    # Extracting the Name\n",
    "    # In case of an error, try changing the tags used here.\n",
    "    name_loc = intro.find(\"h1\")\n",
    "    # strip() is used to remove any extra blank spaces\n",
    "    name = name_loc.get_text().strip()\n",
    "\n",
    "    # This gives us the HTML of the tag in which user status is present\n",
    "    status_loc = intro.find(\"div\", {'class': 'text-body-medium'})\n",
    "    # Extracting user status\n",
    "    status = status_loc.get_text().strip()\n",
    "    \n",
    "    # Extracting the Company name\n",
    "    work_space = soup.find('ul', {'class': 'pv-text-details__right-panel'})\n",
    "    # This gives us the HTML of the tag in which the Company Name is present\n",
    "    works_at_loc = work_space.find('span', {'class': 'pv-text-details__right-panel-item-text'})\n",
    "    # Extracting the Company Name\n",
    "    works_at = works_at_loc.get_text().strip()\n",
    "\n",
    "    # Добавил случайный скроллинг страницы для имитации человека\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    NUM_SCROLLS = random.randint(4, 10)\n",
    "\n",
    "    for _ in range(NUM_SCROLLS):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(random.uniform(1.5, 3))\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    time.sleep(random.uniform(2, 5))\n",
    "    \n",
    "    # Return results for current user\n",
    "    return [name, status, works_at, profile_url]\n",
    "\n",
    "    # Print collected data\n",
    "#     print(\"Name -->\",  name,\n",
    "#           \"\\nStatus -->\", status,\n",
    "#           \"\\nWorks At -->\", works_at,\n",
    "#           \"\\nProfile URL -->\", cur_profile_url)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47867bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_save_users_posts(driver, url):\n",
    "    driver.get(url + '/recent-activity/all/')\n",
    "    time.sleep(random.uniform(4, 7))\n",
    "    SCROLL_PAUSE_TIME = random.uniform(1, 3)\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    # We can adjust this number to get more posts\n",
    "    NUM_SCROLLS = random.randint(7, 10)\n",
    "    for i in range(NUM_SCROLLS):\n",
    "    \n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait to load page\n",
    "        time.sleep(random.uniform(3,6))\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "\n",
    "        last_height = new_height\n",
    "\n",
    "    src = driver.page_source\n",
    "    \n",
    "    # Установил lxml\n",
    "    soup = BeautifulSoup(src, 'lxml')\n",
    "    posts = soup.find_all('li', class_='profile-creator-shared-feed-update__container')\n",
    "\n",
    "    for post_src in posts:\n",
    "        post_source = []\n",
    "        post_source.append(url)\n",
    "\n",
    "        # Текст статьи репоста\n",
    "        post_text_div = post_src.find('a', {'class': 'tap-target update-components-mini-update-v2__link-to-details-page text-body-medium ember-view'})\n",
    "        if post_text_div is not None:\n",
    "            post_text = post_text_div.find('span', {'dir': 'ltr'})\n",
    "        else:\n",
    "            post_text = None\n",
    "\n",
    "        if post_text is not None:\n",
    "            post_text = post_text.get_text().strip()\n",
    "\n",
    "        if post_text is None:\n",
    "            post_text_div = post_src.find('div', {'class': 'feed-shared-update-v2__description-wrapper mr2'})            \n",
    "            if post_text_div is not None:\n",
    "                post_text = post_text_div.find('span', {'dir': 'ltr'})\n",
    "            else:\n",
    "                post_text = None\n",
    "\n",
    "            # If post text is found\n",
    "            if post_text is not None:\n",
    "                post_text = post_text.get_text().strip()\n",
    "            else:\n",
    "                post_text = 'No text'\n",
    "\n",
    "        post_source.append(post_text)\n",
    "\n",
    "        # Подсчет лайков\n",
    "        likes_cnt = post_src.find('span', {'class': 'social-details-social-counts__reactions-count'})\n",
    "\n",
    "        # If number of reactions is written as text\n",
    "        # It has different class name\n",
    "        if likes_cnt is None:\n",
    "            likes_cnt = post_src.find('span', {'class': 'social-details-social-counts__social-proof-text'})\n",
    "\n",
    "\n",
    "        if likes_cnt is not None:\n",
    "            likes_cnt = likes_cnt.get_text().strip()\n",
    "        else:\n",
    "            likes_cnt = 0\n",
    "\n",
    "        post_source.append(likes_cnt)\n",
    "\n",
    "        # Подсчет репостов\n",
    "        reposts_cnt = post_src.find('li', {'class': 'social-details-social-counts__item social-details-social-counts__item--with-social-proof'})\n",
    "        if reposts_cnt is not None:\n",
    "            reposts_cnt = reposts_cnt.find('span', {'aria-hidden': 'true'})\n",
    "        if reposts_cnt is not None:\n",
    "            reposts_cnt = reposts_cnt.get_text().strip()\n",
    "        else:\n",
    "            reposts_cnt = 0 \n",
    "        post_source.append(reposts_cnt)\n",
    "\n",
    "        # Подсчет комментариев\n",
    "        comment_cnt = post_src.find('li', {'class': 'social-details-social-counts__item social-details-social-counts__comments social-details-social-counts__item--with-social-proof'})\n",
    "        if comment_cnt is not None:\n",
    "            comment_cnt = comment_cnt.find('span', {'aria-hidden': 'true'})\n",
    "        if comment_cnt is not None:\n",
    "            comment_cnt = comment_cnt.get_text().strip()\n",
    "        else:\n",
    "            comment_cnt = 0 \n",
    "\n",
    "        post_source.append(comment_cnt)          \n",
    "        \n",
    "        # Сохранение в файл\n",
    "        with open(POSTS_FILE_NAME, 'a', newline='', encoding='utf-8') as file:\n",
    "            writer_obj = csv.writer(file)\n",
    "            writer_obj.writerow(post_source)\n",
    "            \n",
    "        time.sleep(random.uniform(2, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1c3c0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Введите строку поиска: https://www.linkedin.com/search/results/people/?geoUrn=%5B%22104994045%22%5D&keywords=data%20scientist&origin=FACETED_SEARCH&sid=qAI\n"
     ]
    }
   ],
   "source": [
    "# Количество страниц результатов поиска для парсинга\n",
    "NUM_PAGES_TO_PARSE = 2\n",
    "\n",
    "# Название файла cookies\n",
    "COOKIES_PATH = '\\lincookies'\n",
    "\n",
    "# Папка для хранения файлов\n",
    "FILES_PATH = 'C:\\Recruiters'\n",
    "\n",
    "# Путь к файлу с URL\n",
    "URL_FILE_NAME = FILES_PATH + r\"\\urls.csv\"\n",
    "\n",
    "# Путь к файлу с инфо пользователей\n",
    "INFO_FILE_NAME = FILES_PATH + r\"\\user-info.csv\"\n",
    "\n",
    "# Путь к файлу с постами\n",
    "POSTS_FILE_NAME = FILES_PATH + r\"\\posts.csv\"\n",
    "\n",
    "# Суффикс ссылки на посты пользователя\n",
    "POSTS_URL_SUFFIX = 'recent-activity/all/'\n",
    "\n",
    "# Ввод поисковой строки\n",
    "SEARCH_URL = str(input('Введите строку поиска: '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb574cb8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Запуск драйвера для скрэпинга\n",
    "\n",
    "# Проверка наличия папки для файлов\n",
    "if not os.path.exists(FILES_PATH):\n",
    "    os.mkdir(FILES_PATH)\n",
    "\n",
    "caps = DesiredCapabilities().CHROME\n",
    "caps['pageLoadStrategy'] = 'eager'\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Установлен широкоформатный размер экрана, чтобы нужные элементы помещались на экране\n",
    "driver.set_window_size(1920, 1080)\n",
    "\n",
    "# Вызов функции авторизации\n",
    "auth(driver)\n",
    "time.sleep(random.uniform(5, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68a13286",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1\n",
      "Sucсess!\n",
      "--------------------------------------------------\n",
      "Page 2\n",
      "Sucсess!\n",
      "--------------------------------------------------\n",
      "Done!\n",
      "URLs saved to file: C:\\Recruiters\\urls.csv\n"
     ]
    }
   ],
   "source": [
    "# Парсинг URL адресов страниц пользователей и сохранение их в файл\n",
    "profile_urls(driver, SEARCH_URL)\n",
    "time.sleep(random.uniform(5, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b738c8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users info saved to file: C:\\Recruiters\\user-info.csv\n"
     ]
    }
   ],
   "source": [
    "# Загрузка URL's из файла\n",
    "profile_urls = pd.read_csv(URL_FILE_NAME, index_col=0, header=0, names=['user_url'])\n",
    "profile_info = pd.DataFrame({'name': [],\n",
    "     'status': [],\n",
    "     'company': [],\n",
    "     'profile_url': []})\n",
    "\n",
    "# Получение информации из профилей пользователей\n",
    "for profile_url in profile_urls['user_url']:\n",
    "    profile_info.loc[len(profile_info.index)] = get_and_save_profile_info(driver, profile_url)\n",
    "    time.sleep(random.uniform(4, 7))\n",
    "\n",
    "# Сохранение файла с инфо пользователей\n",
    "profile_info.to_csv(INFO_FILE_NAME)\n",
    "print(f'Users info saved to file: {INFO_FILE_NAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "422afc42",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "profile_urls = pd.read_csv(URL_FILE_NAME, index_col=0, header=0, names=['user_url'])\n",
    "\n",
    "#Создаем файл для записи постов\n",
    "header = ['url', 'text', 'likes_cnt', 'reposts_cnt', 'comments_cnt']\n",
    "with open(POSTS_FILE_NAME, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "\n",
    "for url in profile_urls['user_url']:\n",
    "    get_and_save_users_posts(driver, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db7344ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выключаем драйвер скрэппинга\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58624df7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
